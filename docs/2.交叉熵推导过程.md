# 交叉熵的推导过程

---

## 🌟 简短回答（先说结论）：

> 因为逻辑回归是一个 **概率模型**，而交叉熵正是度量**两个概率分布差异**的最自然的方式。  
> 同时，**最大化似然函数**（MLE）的目标，等价于**最小化交叉熵损失**。

---

## 🧠 一、逻辑回归模型的输出是概率

逻辑回归输出的是：

$$
\hat{y} = \sigma(wx + b) = \frac{1}{1 + e^{-(wx + b)}}
$$

这个输出是 **预测为类别 1 的概率值**，在 0 到 1 之间。

而真实标签 $ y $ 是离散值（0 或 1）。所以我们用交叉熵来衡量预测的概率分布与真实分布的差异。

---

## ✍️ 二、从极大似然推导出交叉熵损失

### Step 1：写出似然函数

假设数据集有 $ n $ 个样本，第 $ i $ 个样本的真实标签是 $ y_i \in \{0, 1\} $，预测概率是 $ \hat{y}_i = \sigma(wx_i + b) $

则**每个样本的条件概率**为：

$$
P(y_i | x_i) = \hat{y}_i^{y_i} (1 - \hat{y}_i)^{1 - y_i}
$$

这个式子你可以验证：  
- 如果 $ y_i = 1 $，它就变成 $ \hat{y}_i $；
- 如果 $ y_i = 0 $，它就变成 $ 1 - \hat{y}_i $。

---

### Step 2：写出总体似然函数

$$
L(w, b) = \prod_{i=1}^{n} \hat{y}_i^{y_i} (1 - \hat{y}_i)^{1 - y_i}
$$

为了便于优化，取对数变成对数似然函数：

$$
\log L(w, b) = \sum_{i=1}^{n} \left[ y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

---

### Step 3：最大化对数似然 = 最小化负对数似然

我们一般最小化损失，所以取负号：

$$
\text{Loss} = - \sum_{i=1}^{n} \left[ y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

这就是 **交叉熵损失函数**：

$$
\text{CrossEntropy}(y, \hat{y}) = -\left[ y \log \hat{y} + (1 - y) \log(1 - \hat{y}) \right]
$$

---

## 📊 三、交叉熵的直觉解释

交叉熵衡量的是：
> “我用 $\hat{y}$ 表示的分布，和真实分布 $y$，差别有多大”。

对于分类问题来说：
- 如果预测越接近真实标签（例如 $y = 1, \hat{y} = 0.999$），交叉熵越小；
- 如果预测越偏离（例如 $y = 1, \hat{y} = 0.01$），交叉熵会非常大（因为 log 接近负无穷）；
- 所以它惩罚“信心十足但错得离谱”的预测。

---

## ✅ 总结

| 原理          | 说明                  |
|-------------|---------------------|
| 最大似然估计（MLE） | 推导出交叉熵              |
| 概率解释        | 预测是概率，交叉熵适合度量分布之间差异 |
| 优化效果好       | 数学性质好，导数简洁，梯度可计算    |
| 常用于分类问题     | 尤其是二分类、多分类都常用交叉熵损失  |